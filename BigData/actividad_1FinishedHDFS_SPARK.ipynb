{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bfcfc880f2897e6836933d55c7c042",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 1: HDFS, Spark SQL y MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a238d899b5a6e8c414330ade880233f",
     "grade": false,
     "grade_id": "cell-f4c598b6fd61ee12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallarán y el ejercicio no puntuará. Debe reemplazarse el valor `None` al que están inicializadas por el código necesario para resolver el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10cdca7b33ec4328e542f94942371393",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Leemos el fichero flights.csv que hemos subido a HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b86205b9dc8b69adf7aa9ba76518864",
     "grade": false,
     "grade_id": "cell-3202a483f423590a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\n",
    "ha inferido correctamente o no. La ruta del archivo en HDFS debería ser /<nombre_alumno>/flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5310bc8c3244ee98391957d26dc91d08",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pyspark in /usr/local/miniconda/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/miniconda/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pyspark==2.4.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 MB\u001b[0m \u001b[31m229.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:22\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m477.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.0-py2.py3-none-any.whl size=213793580 sha256=b09089cd34b77a5ba3a813dfe95f365f02f5f4184241d454ac2dda164ccc6659\n",
      "  Stored in directory: /home/vagrant/.cache/pip/wheels/8f/54/1f/ddcc097e08014dc58016b8c74cef78ce2bb2d2cc70ff239d90\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.4.1\n",
      "    Uninstalling pyspark-3.4.1:\n",
      "      Successfully uninstalled pyspark-3.4.1\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pyspark==3.2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d4/0e/4528c9703863fc4df49fd4425c6f15ee5f370cff9e43cea3f8076b034e3f/pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m182.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:33\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.2 (from pyspark==3.2.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/e2/543019a6e620b759a59f134158b4595766f9bf520a1081a2ba1a1809ba32/py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m216.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805894 sha256=5c6b53ee197af8e5070628bbf7f968e4285a7698f7f5f4ab8ba13fb067ff050d\n",
      "  Stored in directory: /home/vagrant/.cache/pip/wheels/57/a5/f0/c310c379786aa444a8136d5e18697f231bed7aa931bf986a0c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.7\n",
      "    Uninstalling py4j-0.10.7:\n",
      "      Successfully uninstalled py4j-0.10.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 2.4.0\n",
      "    Uninstalling pyspark-2.4.0:\n",
      "      Successfully uninstalled pyspark-2.4.0\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting pyspark==2.4.0\n",
      "  Using cached pyspark-2.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.7 (from pyspark==2.4.0)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.2\n",
      "    Uninstalling py4j-0.10.9.2:\n",
      "      Successfully uninstalled py4j-0.10.9.2\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.2.0\n",
      "    Uninstalling pyspark-3.2.0:\n",
      "      Successfully uninstalled pyspark-3.2.0\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #Se instala el JDK8\n",
    "#Instalamos spark con pip\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install pyspark==2.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install pyspark==3.2.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install pyspark==2.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "builder. \\\n",
    "appName('UNIR'). \\\n",
    "config(\"spark.executor.memory\",\"1g\"). \\\n",
    "config(\"spark.driver.memory\",\"2g\"). \\\n",
    "config(\"spark.driver.maxResultSize\",\"1g\"). \\\n",
    "getOrCreate()\n",
    "ruta_hdfs = \"hdfs://localhost:8020/GuillermoGomez/flights.csv\" # Reemplaza esto por la ruta correcta del fichero flights.csv en HDFS\n",
    "#flightsDF = None\n",
    "\n",
    "# Descomentar estas líneas\n",
    "flightsDF = spark.read\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .csv(ruta_hdfs)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162049"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
    "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
    "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
    "las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"SET spark.sql.adaptive.enabled=true\")\n",
    "#cuantos_NA1 = flightsDF.where(F.col(\"dep_time\").isNull()).count()\n",
    "#from pyspark.sql import functions as F\n",
    "\n",
    "#cuantos_NA = flightsDF.where(F.col(\"dep_time\") == \"NA\").count()\n",
    "cuantos_NA = flightsDF.filter(flightsDF[\"dep_time\"] == \"NA\").count()\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
    "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 07:18:20 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
    "    #flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "    flightsLimpiado = flightsLimpiado.filter(flightsDF[nombreColumna] != \"NA\")\n",
    "flightsLimpiado.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
    "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160748"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightsLimpiado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/07 07:18:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar:\n",
    "    # método que crea una columna o reemplaza una existente\n",
    "    #flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType()))\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, flightsConvertido[c].cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", flightsConvertido[\"arr_delay\"].cast(DoubleType()))\n",
    "\n",
    "#flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: double (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
    "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5810b869bd7baccabe0a2952dd0baae1",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide: \n",
    "\n",
    "* Crear un nuevo DataFrame llamado `aeropuertosOrigenDF` que tenga una columna `origin` y que tenga tantas filas como aeropuertos distintos de *origen* existan. ¿Cuántas filas tiene? Almacenar dicho recuento en la variable entera `n_origen`.\n",
    "* Crear un nuevo DataFrame llamado `rutasDistintasDF` que tenga dos columnas `origin`, `dest` y que tenga tantas filas como rutas diferentes existan (es decir, como combinaciones distintas haya entre un origen y un destino). Una vez creado, contar cuántas combinaciones hay, almacenando dicho recuento en la variable entera `n_rutas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fb2ec5d49ff84edae4833eca797068b",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|origin| count|\n",
      "+------+------+\n",
      "|   SEA|107940|\n",
      "|   PDX| 52808|\n",
      "+------+------+\n",
      "\n",
      "+------+----+-----+\n",
      "|origin|dest|count|\n",
      "+------+----+-----+\n",
      "|   SEA| RNO|  157|\n",
      "|   SEA| DTW| 1572|\n",
      "|   SEA| CLE|   40|\n",
      "|   SEA| LAX| 7430|\n",
      "|   PDX| SEA| 2241|\n",
      "|   SEA| BLI|   92|\n",
      "|   PDX| IAH|  986|\n",
      "|   PDX| PHX| 3536|\n",
      "|   SEA| SLC| 3503|\n",
      "|   SEA| SBA|  364|\n",
      "|   SEA| BWI|  319|\n",
      "|   PDX| IAD|  368|\n",
      "|   PDX| SFO| 5090|\n",
      "|   SEA| KOA|  559|\n",
      "|   SEA| JAC|   14|\n",
      "|   PDX| MCI|  364|\n",
      "|   SEA| SJC| 3948|\n",
      "|   SEA| ABQ|  607|\n",
      "|   SEA| SAT|  364|\n",
      "|   PDX| ONT|  914|\n",
      "+------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reemplaza None por el código necesario para calcular sus valores correctos\n",
    "aeropuertosOrigenDF = flightsConvertido.groupBy(flightsConvertido['origin']).count()\n",
    "aeropuertosOrigenDF.show()\n",
    "n_origen = aeropuertosOrigenDF.count()\n",
    "rutasDistintasDF = flightsConvertido.groupBy(flightsConvertido['origin'],flightsConvertido['dest']).count()\n",
    "rutasDistintasDF.show()\n",
    "n_rutas = rutasDistintasDF.count()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f02d6e47bb3cc84e97f31cce091f80b3",
     "grade": true,
     "grade_id": "ejercicio-1-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(n_origen == 2)\n",
    "assert(n_rutas == 115)\n",
    "assert(aeropuertosOrigenDF.count() == n_origen)\n",
    "assert(rutasDistintasDF.count() == n_rutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9229f0b48ed644e35a731b404738edf2",
     "grade": false,
     "grade_id": "cell-2b5f0dea18728fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "* Partiendo de nuevo de `flightsConvertido`, se pide calcular, *sólo para los vuelos que llegan con* ***retraso positivo***, el retraso medio a la llegada de dichos vuelos, para cada aeropuerto de destino. La nueva columna con el retraso medio a la llegada debe llamarse `retraso_medio`. El DF resultante debe estar **ordenado de mayor a menor retraso medio**. El código que calcule esto debería ir encapsulado en una función de Python llamada `retrasoMedio` que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el cálculo descrito anteriormente.\n",
    "\n",
    "* Una vez hecha la función, invocarla pasándole como argumento `flightsConvertido` y almacenar el resultado devuelto en la variable `retrasoMedioDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "097436083d0007a7d99d5108e1a504c9",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dest: string (nullable = true)\n",
      " |-- retraso_medio: double (nullable = true)\n",
      "\n",
      "+----+------------------+\n",
      "|dest|     retraso_medio|\n",
      "+----+------------------+\n",
      "| BOI|             64.75|\n",
      "| HDN|              46.8|\n",
      "| SFO|41.193768844221104|\n",
      "| CLE| 35.74193548387097|\n",
      "| SBA|35.391752577319586|\n",
      "| COS| 35.05607476635514|\n",
      "| BWI|34.585798816568044|\n",
      "| EWR| 33.52972258916777|\n",
      "| DFW| 33.27519181585678|\n",
      "| MIA| 32.66187050359712|\n",
      "| ORD| 32.47909024211299|\n",
      "| BNA| 31.94871794871795|\n",
      "| JFK|31.255884586180713|\n",
      "| JAC|             30.25|\n",
      "| PHL|29.245989304812834|\n",
      "| OGG|27.511111111111113|\n",
      "| IAD|27.430875576036865|\n",
      "| HOU| 27.33009708737864|\n",
      "| LGB| 27.07634730538922|\n",
      "| FAT|26.852589641434264|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#def retrasoMedio(df):\n",
    "  # Filtrar los vuelos que llegan con retraso positivo\n",
    "#  vuelos_retraso_positivo = df[df['arr_delay'] > 0]\n",
    "  \n",
    "  # Calcular el retraso medio a la llegada para cada aeropuerto de destino\n",
    "#  retraso_medio = vuelos_retraso_positivo.groupby(df['dest'])\n",
    "    \n",
    "    #.agg(df.mean(df['arr_delay']).alias('retraso_medio'))\n",
    "    \n",
    "  \n",
    "  # Ordenar el DataFrame resultante de mayor a menor retraso medio\n",
    "#  retraso_medio = retraso_medio.sort_values(by=df['arr_delay'], ascending=False)\n",
    "  \n",
    "  # Renombrar la columna con el retraso medio a la llegada\n",
    "  #retraso_medio = retraso_medio.rename(columns={'arr_delay': 'retraso_medio'})\n",
    "  \n",
    "#  return retraso_medio\n",
    "\n",
    "def retrasoMedio(df):\n",
    "  # Filtrar los vuelos que llegan con retraso positivo\n",
    "    \n",
    "  vuelos_retraso_positivo = df[df['arr_delay'] > 0]\n",
    "  vuelos_retraso_positivo = vuelos_retraso_positivo.toPandas()\n",
    "  \n",
    "  # Calcular el retraso medio a la llegada para cada aeropuerto de destino\n",
    "  retraso_medio = vuelos_retraso_positivo.groupby(['dest'])['arr_delay'].mean().reset_index()\n",
    "  \n",
    "  # Ordenar el DataFrame resultante de mayor a menor retraso medio\n",
    "  retraso_medio = retraso_medio.sort_values(by='arr_delay', ascending=False)\n",
    "  \n",
    "  # Renombrar la columna con el retraso medio a la llegada\n",
    "  retraso_medio = retraso_medio.rename(columns={'arr_delay': 'retraso_medio'})\n",
    "  mySchema = StructType([ StructField(\"dest\", StringType(), True)\\\n",
    "                       ,StructField(\"retraso_medio\", DoubleType(), True)])\n",
    "  sparkDF2 = spark.createDataFrame(retraso_medio,schema=mySchema)\n",
    "  \n",
    "  return sparkDF2\n",
    "\n",
    "# Invocar la función retrasoMedio pasando como argumento flightsConvertido\n",
    "retrasoMedioDF = retrasoMedio(flightsConvertido)\n",
    "\n",
    "\n",
    "#Create DataFrame by changing schema\n",
    "\n",
    "sparkDF2.printSchema()\n",
    "sparkDF2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12aa19d467a0c50adface20495b6cf35",
     "grade": true,
     "grade_id": "ejercicio-2-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lista = retrasoMedio(flightsConvertido).take(3)\n",
    "assert((lista[0].retraso_medio == 64.75) & (lista[0].dest == \"BOI\"))\n",
    "assert((lista[1].retraso_medio == 46.8) & (lista[1].dest == \"HDN\"))\n",
    "assert((round(lista[2].retraso_medio, 2) == 41.19) & (lista[2].dest == \"SFO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESCRIBE AQUÍ TU CÓDIGO PARA MOSTRAR EL CONTENIDO DE retrasoMedioDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4405b7db7180445df5cacc66d82db53",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide: \n",
    "\n",
    "* Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f037d604ad0>\n",
      "[('spark.master', 'local'), ('spark.app.id', 'local-1720151757532'), ('spark.app.submitTime', '1720150042955'), ('spark.executor.id', 'driver'), ('spark.driver.host', '10.0.2.15'), ('spark.driver.port', '44953'), ('spark.app.name', 'pyspark-shell'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.startTime', '1720151757463'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/vagrant/spark-warehouse'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]\n"
     ]
    }
   ],
   "source": [
    "# Incluye aquí los imports que necesites\n",
    "# import ......  \n",
    "\n",
    "# Se importa el StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import lit\n",
    "# Se crean las columnas indexadas con la funcion StringIndexer\n",
    "#indexerMonth = StringIndexer(inputCol=flightsConvertido[\"month\"], outputCol=flightsConvertido[\"monthIndexed\"])\n",
    "#indexerCarrier = StringIndexer(inputCol=flightsConvertido[\"carrier\"], outputCol=flightsConvertido[\"carrierIndexed\"])\n",
    "\n",
    "\n",
    "#indexerMonth = StringIndexer(inputCol=flightsConvertido[\"month\"], outputCol=\"monthIndexed\")\n",
    "#indexerCarrier = StringIndexer(inputCol=flightsConvertido[\"carrier\"], outputCol=\"carrierIndexed\")\n",
    "\n",
    "#indexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\")\n",
    "#indexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\")\n",
    "print(spark)\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "SparkContext._active_spark_context = spark.sparkContext\n",
    "indexerMonth = StringIndexer(). \\\n",
    "setInputCol(\"month\"). \\\n",
    "setOutputCol(\"monthIndexed\") \n",
    "\n",
    "indexerCarrier = StringIndexer(). \\\n",
    "setInputCol(\"carrier\"). \\\n",
    "setOutputCol(\"carrierIndexed\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96c7c7d5836922dd549b358b897b781",
     "grade": true,
     "grade_id": "string-indexer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(indexerMonth, StringIndexer))\n",
    "assert(isinstance(indexerCarrier, StringIndexer))\n",
    "assert(indexerMonth.getInputCol() == \"month\")\n",
    "assert(indexerMonth.getOutputCol() == \"monthIndexed\")\n",
    "assert(indexerCarrier.getInputCol() == \"carrier\")\n",
    "assert(indexerCarrier.getOutputCol() == \"carrierIndexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e024b0baeeb36bb74d136c7e113372",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_785c44eb584e"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Incluye aquí los imports que necesites\n",
    "# import ...........\n",
    "\n",
    "columnas_ensamblar = None\n",
    "vectorAssembler = None\n",
    "\n",
    "# Se importa el vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Recorremos las columnas de entrada\n",
    "columnas_ensamblar = [x for x in flightsConvertido.columns if x in {\"day\", \"dep_time\", \"arr_time\", \"distance\",\"air_time\"}]\n",
    "# Se listan las columnas de entrada con las columnas indexadas\n",
    "columnas_ensamblar = columnas_ensamblar + [\"monthIndexed\"] + [\"carrierIndexed\"]\n",
    "# Se crea el vector de assembler con las columnas ensambladas\n",
    "vectorAssembler = VectorAssembler(inputCols=columnas_ensamblar, outputCol=\"features\")\n",
    "vectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols()\n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eeb3a3f0b15e8b374789706cd9bce49",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n",
    "\n",
    "* Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\n",
    "# import .........\n",
    "\n",
    "delayBinarizer = None\n",
    "\n",
    "# Se importa el binarizador\n",
    "from pyspark.ml.feature import Binarizer\n",
    "# Se utiliza el binarizador considerando un umbral de 15 con threshold\n",
    "delayBinarizer = Binarizer(threshold=15, inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ec408ab263c0378526f96bb5d374704",
     "grade": true,
     "grade_id": "binarizer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(delayBinarizer, Binarizer))\n",
    "assert(delayBinarizer.getThreshold() == 15)\n",
    "assert(delayBinarizer.getInputCol() == \"arr_delay\")\n",
    "assert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5aed16d7ed0219fe1d8741848f594319",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Por último, crearemos el modelo de clasificación.\n",
    "\n",
    "* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n",
    "* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\n",
    "# import .........\n",
    "\n",
    "decisionTree = None\n",
    "\n",
    "# Se importa el modelo de clasificacion DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Se entrena el modelo con la columna crea en el binarizador arr_delay_binary\n",
    "decisionTree = DecisionTreeClassifier(labelCol='arr_delay_binary', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b03b48f304b5b34cd06eeec49e001fd",
     "grade": true,
     "grade_id": "decision-tree-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(isinstance(decisionTree, DecisionTreeClassifier))\n",
    "assert(decisionTree.getFeaturesCol() == \"features\")\n",
    "assert(decisionTree.getLabelCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "728da91edabbac5b62baf31bdd0a707e",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`. \n",
    "\n",
    "* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+--------------+--------------------+----------------+----------------+--------------------+----------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|monthIndexed|carrierIndexed|            features|arr_delay_binary|   rawPrediction|         probability|prediction|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+--------------+--------------------+----------------+----------------+--------------------+----------+\n",
      "|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|        10.0|           0.0|[1.0,1.0,235.0,19...|             1.0|     [30.0,70.0]|           [0.3,0.7]|       1.0|\n",
      "|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|        10.0|           6.0|[1.0,4.0,738.0,25...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|        10.0|           4.0|[1.0,8.0,548.0,20...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|        10.0|           6.0|[1.0,28.0,800.0,2...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|        10.0|           0.0|[1.0,34.0,325.0,2...|             1.0|     [30.0,70.0]|           [0.3,0.7]|       1.0|\n",
      "|2014|    1|  1|      37|       82|     747|     88.0|     DL| N806DN|  1823|   SEA| DTW|     224|    1927|   0|    37|        10.0|           3.0|[1.0,37.0,747.0,2...|             1.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     346|      227|     936|    219.0|     UA| N14219|  1481|   SEA| ORD|     202|    1721|   3|    46|        10.0|           4.0|[1.0,346.0,936.0,...|             1.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     526|       -4|    1148|     15.0|     UA| N813UA|   229|   PDX| IAH|     217|    1825|   5|    26|        10.0|           4.0|[1.0,526.0,1148.0...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     527|        7|     917|     24.0|     UA| N75433|  1576|   SEA| DEN|     136|    1024|   5|    27|        10.0|           4.0|[1.0,527.0,917.0,...|             1.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     536|        1|    1334|     -6.0|     UA| N574UA|   478|   SEA| EWR|     268|    2402|   5|    36|        10.0|           4.0|[1.0,536.0,1334.0...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     541|        1|     911|      4.0|     UA| N36476|  1569|   PDX| DEN|     130|     991|   5|    41|        10.0|           4.0|[1.0,541.0,911.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     549|       24|     907|     12.0|     US| N548UW|   649|   PDX| PHX|     122|    1009|   5|    49|        10.0|           6.0|[1.0,549.0,907.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     550|        0|     837|    -12.0|     DL| N660DL|  1634|   SEA| SLC|      82|     689|   5|    50|        10.0|           3.0|[1.0,550.0,837.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     557|       -3|    1134|    -16.0|     AA| N3JLAA|  1094|   SEA| DFW|     184|    1660|   5|    57|        10.0|           5.0|[1.0,557.0,1134.0...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     557|       -3|     825|    -25.0|     AS| N562AS|    81|   SEA| ANC|     188|    1448|   5|    57|        10.0|           0.0|[1.0,557.0,825.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     558|       -2|     801|     -2.0|     AS| N402AS|   200|   SEA| SJC|     100|     697|   5|    58|        10.0|           0.0|[1.0,558.0,801.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     559|       -1|     916|     -9.0|     F9| N210FR|   796|   PDX| DEN|     125|     991|   5|    59|        10.0|           9.0|[1.0,559.0,916.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     600|        0|    1151|    -19.0|     AA| N3JFAA|  2240|   SEA| ORD|     206|    1721|   6|     0|        10.0|           5.0|[1.0,600.0,1151.0...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     600|      -10|     842|     -8.0|     AS| N786AS|   426|   SEA| LAX|     125|     954|   6|     0|        10.0|           0.0|[1.0,600.0,842.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "|2014|    1|  1|     602|       -3|     943|      5.0|     F9| N201FR|   144|   SEA| DEN|     129|    1024|   6|     2|        10.0|           9.0|[1.0,602.0,943.0,...|             0.0|[48916.0,4154.0]|[0.92172602223478...|       0.0|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+--------------+--------------------+----------------+----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Incluye aquí los imports que necesites y que no hayas incluido ya en alguna celda anterior\n",
    "# import .........\n",
    "\n",
    "pipeline = None\n",
    "pipelineModel = None\n",
    "flightsPredictions = None\n",
    "\n",
    "# Se importa Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "#Se crea el Pipeline en paralelo\n",
    "pipeline = Pipeline(stages=[indexerMonth, indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\n",
    "# Estrenamiento usando el metodo fit\n",
    "pipelineModel = pipeline.fit(flightsConvertido)\n",
    "# Se aplica la predeccion con el metodo transform al Pipeline entrenado\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido)\n",
    "#Se visualiza el DataFrame\n",
    "flightsPredictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "107b5ec260550eb4235ffecff655289a",
     "grade": true,
     "grade_id": "pipeline-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "assert(isinstance(pipeline, Pipeline))\n",
    "assert(len(pipeline.getStages()) == 5)\n",
    "assert(isinstance(pipelineModel, PipelineModel))\n",
    "assert(\"probability\" in flightsPredictions.columns)\n",
    "assert(\"prediction\" in flightsPredictions.columns)\n",
    "assert(\"rawPrediction\" in flightsPredictions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 318:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------+\n",
      "|arr_delay_binary|prediction| count|\n",
      "+----------------+----------+------+\n",
      "|             1.0|       1.0|   495|\n",
      "|             0.0|       1.0|    61|\n",
      "|             1.0|       0.0| 23754|\n",
      "|             0.0|       0.0|136438|\n",
      "+----------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
